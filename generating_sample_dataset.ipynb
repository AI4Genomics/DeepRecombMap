{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'write'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7a027ecd6cb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# tqdm call to sys.stdout must be done BEFORE stdout redirection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# and you need to specify sys.stdout, not sys.stderr (default)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#sys.stdout):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnostdout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mblabla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_py3/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, iterable, desc, total, leave, file, ncols, mininterval, maxinterval, miniters, ascii, disable, unit, unit_scale, dynamic_ncols, smoothing, bar_format, initial, position, postfix, unit_divisor, write_bytes, lock_args, nrows, gui, **kwargs)\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# Initialize the screen printer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_printer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;31m# Init the time counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_py3/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_py3/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1467\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1468\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1469\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1470\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_py3/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36mprint_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mlen_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m             \u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_py3/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36mfp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m             \u001b[0mfp_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'write'"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "import contextlib\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DummyFile(object):\n",
    "    file = None\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "\n",
    "    def write(self, x):\n",
    "        # Avoid print() second call (useless \\n)\n",
    "        if len(x.rstrip()) > 0:\n",
    "            tqdm.write(x, file=self.file)\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def nostdout():\n",
    "    save_stdout = sys.stdout\n",
    "    sys.stdout = DummyFile(sys.stdout)\n",
    "    yield\n",
    "    sys.stdout = save_stdout\n",
    "\n",
    "def blabla():\n",
    "    print(\"Foo blabla\")\n",
    "\n",
    "# tqdm call to sys.stdout must be done BEFORE stdout redirection\n",
    "# and you need to specify sys.stdout, not sys.stderr (default)\n",
    "for _ in tqdm(range(3), file=sys.stdout):\n",
    "    with nostdout():\n",
    "        blabla()\n",
    "        sleep(.5)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "processed: 1:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "processed: 2:  33%|███▎      | 1/3 [00:01<00:02,  1.00s/it]\u001b[A\n",
      "processed: 2:  67%|██████▋   | 2/3 [00:01<00:00,  1.99it/s]\u001b[A\n",
      "processed: 3:  67%|██████▋   | 2/3 [00:02<00:00,  1.99it/s]\u001b[A\n",
      "processed: 3: 100%|██████████| 3/3 [00:03<00:00,  1.01s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "values = range(3)\n",
    "with tqdm(total=len(values), file=sys.stdout) as pbar:\n",
    "    for i in values:\n",
    "        pbar.set_description('processed: %d' % (1 + i))\n",
    "        pbar.update(1)\n",
    "        sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of datasets in this file:\n",
      "  target_labels\n",
      "  test_headers\n",
      "  test_in\n",
      "  test_out\n",
      "  train_in\n",
      "  train_out\n",
      "  valid_in\n",
      "  valid_out\n",
      "\n",
      "Shape of the input training data is: (530925, 4, 1, 600)\n",
      "Shape of the output training data is: (530925, 164)\n",
      "Shape of the input velidation data is: (70000, 4, 1, 600)\n",
      "Shape of the output velidation data is: (70000, 164)\n",
      "Shape of the input test data is: (71886, 4, 1, 600)\n",
      "Shape of the output test data is: (71886, 164)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "sample_training_size, sample_valid_size, sample_test_size = 3200, 400, 400\n",
    "\n",
    "sample_h5_file = {}\n",
    "\n",
    "with h5py.File('data/sample_dataset.h5', 'w') as hdf_w:\n",
    "    with h5py.File('data/er.h5', 'r') as hdf:\n",
    "        ls = list(hdf.keys())\n",
    "        print('List of datasets in this file:\\n  {}\\n'.format(\"\\n  \".join(ls)))\n",
    "\n",
    "        hdf_w.create_dataset('target_labels', data=list(hdf['target_labels']))\n",
    "         \n",
    "        train_in = hdf.get('train_in')\n",
    "        train_out = hdf.get('train_out')\n",
    "        print(\"Shape of the input training data is:\", train_in.shape)\n",
    "        print(\"Shape of the output training data is:\", train_out.shape)\n",
    "        indx = np.random.choice(range(len(train_in)), sample_training_size, replace=False)\n",
    "        hdf_w.create_dataset('train_in', data=train_in[sorted(list(indx))])\n",
    "        hdf_w.create_dataset('train_out', data=train_out[sorted(list(indx))])\n",
    "\n",
    "        valid_in = hdf.get('valid_in')\n",
    "        valid_out = hdf.get('valid_out')\n",
    "        print(\"Shape of the input velidation data is:\", valid_in.shape)\n",
    "        print(\"Shape of the output velidation data is:\", valid_out.shape)\n",
    "        indx = np.random.choice(range(len(valid_in)), sample_valid_size, replace=False)\n",
    "        hdf_w.create_dataset('valid_in', data=valid_in[sorted(list(indx))])\n",
    "        hdf_w.create_dataset('valid_out', data=valid_out[sorted(list(indx))])\n",
    "\n",
    "        test_in = hdf.get('test_in')\n",
    "        test_out = hdf.get('test_out')\n",
    "        print(\"Shape of the input test data is:\", test_in.shape)\n",
    "        print(\"Shape of the output test data is:\", test_out.shape)\n",
    "        indx = np.random.choice(range(len(test_in)), sample_test_size, replace=False)\n",
    "        hdf_w.create_dataset('test_in', data=test_in[sorted(list(indx))])\n",
    "        hdf_w.create_dataset('test_out', data=test_out[sorted(list(indx))])\n",
    "        test_headers_org = list(hdf['test_headers'])\n",
    "        test_headers = [test_headers_org[i] for i in sorted(list(indx))]\n",
    "        hdf_w.create_dataset('test_headers', data=test_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of datasets in this file:\n",
      "  target_labels\n",
      "  test_headers\n",
      "  test_in\n",
      "  test_out\n",
      "  train_in\n",
      "  train_out\n",
      "  valid_in\n",
      "  valid_out\n",
      "\n",
      "Shape of the input training data is: (3200, 4, 1, 600)\n",
      "Shape of the output training data is: (3200, 164)\n",
      "Shape of the input velidation data is: (400, 4, 1, 600)\n",
      "Shape of the output velidation data is: (400, 164)\n",
      "Shape of the input test data is: (400, 4, 1, 600)\n",
      "Shape of the output test data is: (400, 164)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/sample_dataset.h5', 'r') as hdf:\n",
    "    ls = list(hdf.keys())\n",
    "    print('List of datasets in this file:\\n  {}\\n'.format(\"\\n  \".join(ls)))\n",
    "\n",
    "    train_in = hdf.get('train_in')\n",
    "    train_out = hdf.get('train_out')\n",
    "    print(\"Shape of the input training data is:\", train_in.shape)\n",
    "    print(\"Shape of the output training data is:\", train_out.shape)\n",
    "\n",
    "    valid_in = hdf.get('valid_in')\n",
    "    valid_out = hdf.get('valid_out')\n",
    "    print(\"Shape of the input velidation data is:\", valid_in.shape)\n",
    "    print(\"Shape of the output velidation data is:\", valid_out.shape)\n",
    "\n",
    "    test_in = hdf.get('test_in')\n",
    "    test_out = hdf.get('test_out')\n",
    "    print(\"Shape of the input test data is:\", test_in.shape)\n",
    "    print(\"Shape of the output test data is:\", test_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BassetDataset(Dataset):\n",
    "\n",
    "    # Initializes the BassetDataset\n",
    "    def __init__(self, path='./data/', f5name='sample_dataset.h5', split='train', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            :param path: path to HDF5 file\n",
    "            :param f5name: HDF5 file name\n",
    "            :param split: split that we are interested to work with\n",
    "            :param transform (callable, optional): Optional transform to be applied on a sample\n",
    "        \"\"\"\n",
    "        \n",
    "        self.split = split\n",
    "        \n",
    "        split_dict = {'train': ['train_in', 'train_out'], \n",
    "                      'test': ['test_in', 'test_out'], \n",
    "                      'valid': ['valid_in', 'valid_out']}\n",
    "        \n",
    "        assert self.split in split_dict, \"'split' argument can be only defined as 'train', 'valid' or 'test'\"\n",
    "        \n",
    "        # Open hdf5 file where one-hoted data are stored\n",
    "        self.dataset = h5py.File(os.path.join(path, f5name.format(self.split)), 'r')\n",
    "        \n",
    "        # Keeping track of the names of the target labels\n",
    "        self.target_labels = self.dataset['target_labels']\n",
    "        \n",
    "        # Get the list of volumes\n",
    "        self.inputs = self.dataset[split_dict[split][0]]\n",
    "        self.outputs = self.dataset[split_dict[split][1]]\n",
    "        if self.split!='test':\n",
    "            self.ids = list(range(len(self.inputs)))\n",
    "        else:\n",
    "            self.ids = np.char.decode(self.dataset['test_headers'])\n",
    "            \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        id = self.ids[i]\n",
    "\n",
    "        # Sequence & Target\n",
    "        sequence, target = self.inputs[id], self.outputs[id]\n",
    "\n",
    "        return sequence, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples in train split is 3200.\n",
      "\n",
      "The number of samples in valid split is 400.\n",
      "\n",
      "The number of samples in test split is 400.\n",
      "The first 10 ids of test samples are:\n",
      "  chr4:160811195-160811795(+)\n",
      "  chr4:110154618-110155218(+)\n",
      "  chr1:182525355-182525955(+)\n",
      "  chr4:104167020-104167620(+)\n",
      "  chr13:45964352-45964952(+)\n",
      "  chr12:107869980-107870580(+)\n",
      "  chr12:3902440-3903040(+)\n",
      "  chr1:143171000-143171600(+)\n",
      "  chr12:125819735-125820335(+)\n",
      "  chr1:23439640-23440240(+)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# path='./data/', f5name='sample_dataset.h5', split='test'\n",
    "basset_dataset_train = BassetDataset(path='./data/', f5name='sample_dataset.h5', split='train')\n",
    "print(\"The number of samples in {} split is {}.\\n\".format('train', len(basset_dataset_train)))\n",
    "\n",
    "basset_dataset_train = BassetDataset(path='./data/', f5name='sample_dataset.h5', split='valid')\n",
    "print(\"The number of samples in {} split is {}.\\n\".format('valid', len(basset_dataset_train)))\n",
    "\n",
    "basset_dataset_test = BassetDataset(path='./data/', f5name='sample_dataset.h5', split='test')\n",
    "print(\"The number of samples in {} split is {}.\".format('test', len(basset_dataset_test)))\n",
    "print(\"The first 10 ids of test samples are:\\n  {}\\n\".format(\"\\n  \".join(basset_dataset_test.ids[:10])))\n",
    "\n",
    "batch_size = 64\n",
    "basset_dataloader_train = DataLoader(basset_dataset, batch_size=batch_size, drop_last=True, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs, trgs = next(iter(basset_dataloader_train)) # Training, validation and test would done on seqs, trgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the batch for input: torch.Size([64, 4, 1, 600])\n",
      "Shape of the batch for output: torch.Size([64, 164])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the batch for input: {}\".format(seqs.shape))\n",
    "print(\"Shape of the batch for output: {}\".format(trgs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
